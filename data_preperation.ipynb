{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 1. Setup and Initial Imports\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import os \n",
    "\n",
    "# Define constants for file paths\n",
    "OZONE_PATH_2001_2005 = \"/mnt/c/Users/WSTATION/Desktop/GEOAI_ML/data/Daily_Census_Tract-Level_Ozone_Concentrations_2001-2005.csv\"\n",
    "OZONE_PATH_2006_2010 = \"/mnt/c/Users/WSTATION/Desktop/GEOAI_ML/data/Daily_Census_Tract-Level_Ozone_Concentrations_2006-2010.csv\"\n",
    "OZONE_PATH_2011_2014 = \"/mnt/c/Users/WSTATION/Desktop/GEOAI_ML/data/Daily_Census_Tract-Level_Ozone_Concentrations_2011-2014.csv\"\n",
    "SHP_PATH = \"/mnt/c/Users/WSTATION/Desktop/GEOAI_ML/data/counties_shapefile/tl_2010_us_county10.shp\"\n",
    "OUTPUT_DIR = \"/mnt/c/Users/WSTATION/Desktop/GEOAI/project/homework/homework/data/clean_data\" # For saving processed files\n",
    "\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete. File paths are defined.\")\n",
    "\n",
    "# NOTE: Will provide the original CSV files seperately through download link given how big they are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c4ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 1. Load Raw Ozone Data\n",
    "# Description: Loading the three separate ozone datasets\n",
    "# ==============================================================================\n",
    "df_ozone_01_05 = pd.read_csv(OZONE_PATH_2001_2005)\n",
    "df_ozone_06_10 = pd.read_csv(OZONE_PATH_2006_2010)\n",
    "df_ozone_11_14 = pd.read_csv(OZONE_PATH_2011_2014)\n",
    "\n",
    "print(\"Raw ozone datasets loaded.\")\n",
    "\n",
    "# Display head for each loaded dataframe\n",
    "print(\"\\n---------- df_ozone_01_05 (2001-2005) Head ----------\")\n",
    "print(tabulate(df_ozone_01_05.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "print(\"\\n---------- df_ozone_06_10 (2006-2010) Head ----------\")\n",
    "print(tabulate(df_ozone_06_10.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "print(\"\\n---------- df_ozone_11_14 (2011-2014) Head ----------\")\n",
    "print(tabulate(df_ozone_11_14.head(), headers='keys', tablefmt='pretty', showindex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4acde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 2. Initial Inspection and Standardization of Ozone DataFrames\n",
    "# Description: Standardizing column names, date formats, and creating a consistent\n",
    "#              5-digit county identifier (GEOID10). Inspecting key ozone columns.\n",
    "# ==============================================================================\n",
    "\n",
    "# Store dataframes in a dictionary for easier batch processing\n",
    "ozone_dfs = {\n",
    "    \"01_05\": df_ozone_01_05,\n",
    "    \"06_10\": df_ozone_06_10,\n",
    "    \"11_14\": df_ozone_11_14\n",
    "}\n",
    "\n",
    "processed_ozone_dfs = {}\n",
    "\n",
    "for key, df_original in ozone_dfs.items():\n",
    "    df = df_original.copy() # Work on a copy\n",
    "    print(f\"\\n--- Processing DataFrame for period: {key} ---\")\n",
    "\n",
    "    # Standardize GEOID10 creation\n",
    "    if key == \"06_10\":   \n",
    "        # For 2006-2010 data: statefips (2-digit) + countyfips (3-digit part)\n",
    "        df['statefips_str'] = df['statefips'].astype(str).str.zfill(2)\n",
    "        df['countyfips_part_str'] = df['countyfips'].astype(str).str.zfill(3)\n",
    "        df['GEOID10'] = df['statefips_str'] + df['countyfips_part_str']\n",
    "        # Drop temporary columns\n",
    "        df.drop(columns=['statefips_str', 'countyfips_part_str'], inplace=True)\n",
    "        print(f\"Created GEOID10 for {key} by combining state and county FIPS parts.\")\n",
    "    else:\n",
    "        # For 2001-2005 and 2011-2014 data: countyfips is the 5-digit FIPS\n",
    "        df['GEOID10'] = df['countyfips'].astype(str).str.zfill(5)\n",
    "        print(f\"Standardized GEOID10 for {key} from existing 5-digit countyfips.\")\n",
    "\n",
    "    # Rename ozone prediction/std columns if necessary (specifically for 2011-2014 df)\n",
    "    if 'ds_o3_pred' in df.columns:  \n",
    "        df.rename(columns={'ds_o3_pred': 'DS_O3_pred', 'ds_o3_stdd': 'DS_O3_stdd'}, inplace=True)\n",
    "        print(f\"Standardized ozone column names for {key}.\")\n",
    "\n",
    "    # Convert 'date' column to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    print(f\"Converted 'date' column to datetime for {key}. Dtype: {df['date'].dtype}\")\n",
    "\n",
    "    # Inspect DS_O3_pred column\n",
    "    print(f\"\\nInspecting DS_O3_pred for {key}:\")\n",
    "    if 'DS_O3_pred' in df.columns:\n",
    "        print(f\"  Missing values: {df['DS_O3_pred'].isnull().sum()}\")\n",
    "        print(f\"  Descriptive statistics:\\n{df['DS_O3_pred'].describe(percentiles=[.01, .05, .25, .5, .75, .95, .99])}\")\n",
    "    else:\n",
    "        print(f\"  DS_O3_pred column not found in {key} after potential rename.\")\n",
    "\n",
    "    # Check unique GEOID10 counts per year\n",
    "    print(f\"\\nUnique GEOID10 counts per year for {key}:\")\n",
    "    print(df.groupby('year')['GEOID10'].nunique())\n",
    "    \n",
    "    processed_ozone_dfs[key] = df\n",
    "\n",
    "# Assign back to original variable names for clarity if needed later or keep in dict\n",
    "df_ozone_01_05_processed = processed_ozone_dfs[\"01_05\"]\n",
    "df_ozone_06_10_processed = processed_ozone_dfs[\"06_10\"]\n",
    "df_ozone_11_14_processed = processed_ozone_dfs[\"11_14\"]\n",
    "\n",
    "print(\"\\n--- Standardized DataFrames Preview ---\")\n",
    "print(\"\\n---------- df_ozone_01_05_processed Head ----------\")\n",
    "print(tabulate(df_ozone_01_05_processed.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "print(\"\\n---------- df_ozone_06_10_processed Head ----------\")\n",
    "print(tabulate(df_ozone_06_10_processed.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "print(\"\\n---------- df_ozone_11_14_processed Head ----------\")\n",
    "print(tabulate(df_ozone_11_14_processed.head(), headers='keys', tablefmt='pretty', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 3. Final Column Cleaning of Ozone DataFrames\n",
    "# Description: Dropping original FIPS columns now that GEOID10 is the standard\n",
    "# ==============================================================================\n",
    "\n",
    "# Update the dictionary keys to reflect processed dataframes\n",
    "ozone_dfs_processed_for_final_cleaning = {\n",
    "    \"01_05\": df_ozone_01_05_processed,\n",
    "    \"06_10\": df_ozone_06_10_processed,\n",
    "    \"11_14\": df_ozone_11_14_processed\n",
    "}\n",
    "\n",
    "for key, df in ozone_dfs_processed_for_final_cleaning.items():\n",
    "\n",
    "    cols_to_drop = ['countyfips', 'statefips']\n",
    "    existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "    if existing_cols_to_drop:\n",
    "        df.drop(columns=existing_cols_to_drop, inplace=True, errors='ignore')\n",
    "        print(f\"Dropped original FIPS columns {existing_cols_to_drop} from DataFrame for period: {key}\")\n",
    "\n",
    "    print(f\"\\n--- {key} DataFrame after final column cleaning ---\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(tabulate(df.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "    \n",
    "    mem_usage_gb = df.memory_usage(deep=True).sum() / (1024 ** 3)\n",
    "    print(f\"Memory usage for {key} (processed): {mem_usage_gb:.3f} GB\")\n",
    "\n",
    "\n",
    "df_ozone_01_05_final = ozone_dfs_processed_for_final_cleaning[\"01_05\"]\n",
    "df_ozone_06_10_final = ozone_dfs_processed_for_final_cleaning[\"06_10\"]\n",
    "df_ozone_11_14_final = ozone_dfs_processed_for_final_cleaning[\"11_14\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 4. Save Processed Ozone DataFrames\n",
    "# Description: Saving the cleaned and standardized ozone data back to feather files.\n",
    "# ==============================================================================\n",
    "df_ozone_01_05_final.to_feather(os.path.join(OUTPUT_DIR, \"ozone_2001_2005_processed.feather\"))\n",
    "df_ozone_06_10_final.to_feather(os.path.join(OUTPUT_DIR, \"ozone_2006_2010_processed.feather\"))\n",
    "df_ozone_11_14_final.to_feather(os.path.join(OUTPUT_DIR, \"ozone_2011_2014_processed.feather\"))\n",
    "\n",
    "print(\"\\nProcessed ozone DataFrames saved to feather files in:\", OUTPUT_DIR)\n",
    "\n",
    "##### IMPORTANT ######\n",
    "# AFTER RUNNING THIS CELL CLEAR ALL OUTPUTS AND RESTART. \n",
    "# BY THIS POINT, YOU WOULD HAVE ACCUMULATED 130GB IN SYSTEM RAM. YOU MUST CLEAR MEMORY\n",
    "# AFTER CLEARING MEMORY, RUN CELL 1 AGAIN, THEN PROCEED TO THE CELL BELOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09465a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# ## 5. Verification: Re-load and Inspect Saved Files\n",
    "# Description: Re-loading the processed files to ensure they were saved correctly\n",
    "# ================================================================================\n",
    "df_reloaded_01_05 = pd.read_feather(os.path.join(OUTPUT_DIR, \"ozone_2001_2005_processed.feather\"))\n",
    "df_reloaded_06_10 = pd.read_feather(os.path.join(OUTPUT_DIR, \"ozone_2006_2010_processed.feather\"))\n",
    "df_reloaded_11_14 = pd.read_feather(os.path.join(OUTPUT_DIR, \"ozone_2011_2014_processed.feather\"))\n",
    "\n",
    "print(\"\\n--- Verification: Re-loaded DataFrames ---\")\n",
    "print(\"\\nRe-loaded df_ozone_01_05 Head & Dtypes:\")\n",
    "print(tabulate(df_reloaded_01_05.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "print(df_reloaded_01_05.dtypes)\n",
    "\n",
    "print(\"\\nRe-loaded df_ozone_06_10 Head & Dtypes:\")\n",
    "print(tabulate(df_reloaded_06_10.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "print(df_reloaded_06_10.dtypes)\n",
    "\n",
    "print(\"\\nRe-loaded df_ozone_11_14 Head & Dtypes:\")\n",
    "print(tabulate(df_reloaded_11_14.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "print(df_reloaded_11_14.dtypes)\n",
    "\n",
    "print(\"\\nVerification step complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9706d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 6. Detailed Inspection of DS_O3_pred and GEOID10 Coverage\n",
    "# Description: Performing statistical summaries for the primary ozone\n",
    "#              measurement column (DS_O3_pred) and verifying the spatial coverage\n",
    "#              (unique GEOID10 counts per year) for each ozone data period\n",
    "# ==============================================================================\n",
    "import pandas as pd \n",
    "from tabulate import tabulate # For pretty printing dataframes\n",
    "\n",
    "\n",
    "\n",
    "ozone_dataframes_for_inspection = {\n",
    "    \"2001-2005\": df_reloaded_01_05, \n",
    "    \"2006-2010\": df_reloaded_06_10, \n",
    "    \"2011-2014\": df_reloaded_11_14\n",
    "}\n",
    "\n",
    "for period, df in ozone_dataframes_for_inspection.items():\n",
    "    print(f\"\\n--- Inspection for Ozone Data: Period {period} ---\")\n",
    "    \n",
    "    # Inspect DS_O3_pred column\n",
    "    if 'DS_O3_pred' in df.columns:\n",
    "        print(f\"Missing values in DS_O3_pred: {df['DS_O3_pred'].isnull().sum()}\")\n",
    "        print(\"Descriptive statistics for DS_O3_pred:\")\n",
    "        print(df['DS_O3_pred'].describe(percentiles=[.01, .05, .25, .5, .75, .95, .99]))\n",
    "    else:\n",
    "        print(f\"DS_O3_pred column not found in DataFrame for period {period}.\")\n",
    "        \n",
    "    # Verify unique GEOID10 counts per year\n",
    "    if 'GEOID10' in df.columns and 'year' in df.columns:\n",
    "        print(\"\\nUnique GEOID10 counts per year:\")\n",
    "        print(df.groupby('year')['GEOID10'].nunique())\n",
    "    else:\n",
    "        print(f\"GEOID10 or year column not found for period {period} for GEOID10 count check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac58ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 7. Load and Subset Counties Shapefile \n",
    "# Description: Loading county boundary data for potential geospatial analysis\n",
    "#              This step is separate from the main ozone data processing flow\n",
    "# ==============================================================================\n",
    "import geopandas as gpd # Placed here as it's specific to this section\n",
    "\n",
    "try:\n",
    "    counties_gdf = gpd.read_file(SHP_PATH)\n",
    "\n",
    "    print(\"\\n--- Counties Shapefile Subset Head ---\")\n",
    "    print(tabulate(counties_gdf.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "except Exception as e:\n",
    "    print(f\"Could not load or process shapefile: {e}\")\n",
    "\n",
    "# Keep only relevant columns (GEOID10, geometry, STATEFP10 renamed to statefips)\n",
    "counties_gdf_subset = counties_gdf[['GEOID10', 'STATEFP10', 'NAME10', 'geometry']].rename(columns={'STATEFP10': 'statefips'})\n",
    "\n",
    "# Merge ozone data (df_reloaded_01_05) with the shapefile data on GEOID\n",
    "merged_gdf = pd.merge(df_reloaded_01_05, counties_gdf_subset, left_on='GEOID10', right_on='GEOID10', how='left')\n",
    "\n",
    "# Convert the merged DataFrame back to a GeoDataFrame\n",
    "merged_gdf = gpd.GeoDataFrame(merged_gdf, geometry='geometry', crs=counties_gdf.crs)\n",
    "\n",
    "# Display to confirm the merge\n",
    "print(merged_gdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c9b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 8. Investigation of Extreme Ozone Event (2001-2005 Data)\n",
    "# Description: Focusing on the 2001-2005 dataset to identify, detail, and\n",
    "#              visualize an anomalous high ozone event, particularly in Missouri\n",
    "#              on September 13, 2001\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd \n",
    "import numpy as np \n",
    "import matplotlib.patheffects as path_effects # For text effects on map\n",
    "import matplotlib.animation as animation # For animation\n",
    "from IPython.display import HTML # For displaying animation in Jupyter\n",
    "\n",
    "# --- 2.1 Identify and Detail Extreme Ozone Values ---\n",
    "# Investigating extreme DS_O3_pred values in the 2001-2005 ozone dataset.\n",
    "print(\"\\n--- Investigating extreme DS_O3_pred values in df_ozone_2001_2005 ---\")\n",
    "# Define a threshold for extreme values\n",
    "# The maximum observed in this dataset was around 491 ppb\n",
    "df_extreme_o3_early = merged_gdf[merged_gdf['DS_O3_pred'] > 140]\n",
    "print(f\"Number of records with DS_O3_pred > 140 ppb in 2001-2005 data: {len(df_extreme_o3_early)}\")\n",
    "\n",
    "if not df_extreme_o3_early.empty:\n",
    "    print(\"Details of extreme ozone values (2001-2005, sorted by DS_O3_pred):\")\n",
    "    # Displaying key columns for the identified extreme records.\n",
    "    cols_to_show = ['date', 'GEOID10', 'statefips', 'latitude', 'longitude', 'DS_O3_pred', 'DS_O3_stdd']\n",
    "    actual_cols_to_show = [col for col in cols_to_show if col in df_extreme_o3_early.columns]\n",
    "    print(tabulate(df_extreme_o3_early[actual_cols_to_show].sort_values(by='DS_O3_pred', ascending=False).head(20), # Show top 20\n",
    "                   headers='keys', tablefmt='pretty', showindex=False))\n",
    "    \n",
    "    print(f\"\\nUnique GEOID10s with extreme ozone (>140 ppb): {df_extreme_o3_early['GEOID10'].nunique()}\")\n",
    "    # Further filtering for the specific known anomaly\n",
    "    df_anomaly_event = df_extreme_o3_early[\n",
    "        (df_extreme_o3_early['date'] == pd.Timestamp('2001-09-13')) &\n",
    "        (df_extreme_o3_early['statefips'] == '29') # Missouri\n",
    "    ]\n",
    "    print(f\"\\nNumber of extreme records (>140ppb) in Missouri on 2001-09-13: {len(df_anomaly_event)}\")\n",
    "    if not df_anomaly_event.empty:\n",
    "         print(\"Details of Missouri anomaly event (2001-09-13):\")\n",
    "         print(tabulate(df_anomaly_event[actual_cols_to_show].sort_values(by='DS_O3_pred', ascending=False),\n",
    "                        headers='keys', tablefmt='pretty', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8.2.2 Spatial Visualization of Extreme Ozone Event in Missouri ---\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate \n",
    "import matplotlib.patheffects as path_effects\n",
    "\n",
    "\n",
    "\n",
    "# 1. Filter the county shapefile for Missouri (STATEFP10 == '29')\n",
    "# -------------------------------------------------------------\n",
    "missouri_fp = '29'\n",
    "gdf_missouri_counties = counties_gdf[counties_gdf['STATEFP10'] == missouri_fp].copy()\n",
    "\n",
    "if gdf_missouri_counties.empty:\n",
    "    print(f\"No counties found for STATEFP10 = {missouri_fp}\")\n",
    "else:\n",
    "    print(f\"\\nFiltered for Missouri counties. Found {len(gdf_missouri_counties)} counties.\")\n",
    "\n",
    "    if 'df_anomaly_event' in locals() and hasattr(df_anomaly_event, 'crs') and df_anomaly_event.crs:\n",
    "        if gdf_missouri_counties.crs != df_anomaly_event.crs:\n",
    "            print(f\"Transforming Missouri counties CRS from {gdf_missouri_counties.crs} to {df_anomaly_event.crs}...\")\n",
    "            gdf_missouri_counties = gdf_missouri_counties.to_crs(df_anomaly_event.crs)\n",
    "    else:\n",
    "        print(\"Warning: gdf_extreme_o3_early is not defined or has no CRS\")\n",
    "\n",
    "\n",
    "    # 3. Plot Missouri Counties and Overlay Extreme Ozone Points\n",
    "    # --------------------------------------------------------\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "    # Plot Missouri county boundaries\n",
    "    gdf_missouri_counties.plot(ax=ax, facecolor='#eaeaea', edgecolor='black', linewidth=0.4)\n",
    "\n",
    "    # Plot ozone points filtered for Missouri\n",
    "    mo_o3 = df_anomaly_event[df_anomaly_event['statefips'] == '29']\n",
    "    mo_o3.plot(\n",
    "        ax=ax,\n",
    "        column='DS_O3_pred',\n",
    "        cmap='OrRd',\n",
    "        markersize=mo_o3['DS_O3_pred'] / 8,\n",
    "        edgecolor='black',\n",
    "        linewidth=0.4,\n",
    "        alpha=0.8,\n",
    "        legend=True,\n",
    "        legend_kwds={\n",
    "            'label': \"DS_O3_pred (ppb)\",\n",
    "            'orientation': \"horizontal\",\n",
    "            'shrink': 0.5,\n",
    "            'pad': 0.05,\n",
    "            'aspect': 30,\n",
    "            'anchor': (0.5, -0.2)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Label only counties with significant ozone points to avoid clutter\n",
    "    counties_to_label = gdf_missouri_counties[\n",
    "        gdf_missouri_counties.intersects(mo_o3.unary_union)\n",
    "    ]\n",
    "\n",
    "    # Annotate counties using centroids\n",
    "    for idx, row in counties_to_label.iterrows():\n",
    "        centroid = row['geometry'].centroid\n",
    "        plt.annotate(\n",
    "            text=row['NAME10'],\n",
    "            xy=(centroid.x, centroid.y),\n",
    "            horizontalalignment='center',\n",
    "            fontsize=7,\n",
    "            color='black',\n",
    "            alpha=0.8,\n",
    "            path_effects=[plt.matplotlib.patheffects.withStroke(linewidth=1, foreground=\"white\")]\n",
    "        )\n",
    "\n",
    "    # Set axes limits\n",
    "    ax.set_xlim([-95.6, -89.2])\n",
    "    ax.set_ylim([36.4, 40.4])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Title and labels\n",
    "    ax.set_title('Extreme Ozone Values on 2001-09-13 Over Missouri Counties', fontsize=16, pad=15)\n",
    "    ax.set_xlabel(\"Longitude\", fontsize=13)\n",
    "    ax.set_ylabel(\"Latitude\", fontsize=13)\n",
    "\n",
    "    # Grid\n",
    "    ax.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "    # Improved layout\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8.2.3 Temporal Visualization of Ozone in Missouri around the Event ---\n",
    "# filtering data for Missouri around the spike of September 13, 2001\n",
    "#date range is defined for September and part of October 2001 to see context\n",
    "date_range_spike_viz = pd.date_range(start='2001-09-01', end='2001-10-17')\n",
    "mo_o3_for_timeseries = merged_gdf[\n",
    "    (merged_gdf['statefips'] == '29') & # Missouri\n",
    "    (merged_gdf['date'].isin(date_range_spike_viz))\n",
    "]\n",
    "\n",
    "if not mo_o3_for_timeseries.empty:\n",
    "    print(\"\\nPlotting daily ozone statistics for Missouri around Sep-Oct 2001\")\n",
    "    # Computing daily statistics for the filtered Missouri data\n",
    "    daily_stats = mo_o3_for_timeseries.groupby('date')['DS_O3_pred'].agg(['mean', 'median', 'max', 'min', 'std']).reset_index()\n",
    "    # Plotting the data clearly\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Mean Ozone\n",
    "    plt.plot(daily_stats['date'], daily_stats['mean'], '-o', color='blue', label='Mean DS_O3_pred')\n",
    "    # Median Ozone\n",
    "    plt.plot(daily_stats['date'], daily_stats['median'], '--s', color='green', label='Median DS_O3_pred')\n",
    "    # Max Ozone\n",
    "    plt.plot(daily_stats['date'], daily_stats['max'], '-^', color='red', linewidth=2, label='Max DS_O3_pred')\n",
    "    # spike on September 13, 2001\n",
    "    spike_date = pd.Timestamp('2001-09-13')\n",
    "    plt.axvline(x=spike_date, color='purple', linestyle='--', linewidth=1.5, alpha=0.8, label='Sept 13 spike')\n",
    "    # Labels, title, legend\n",
    "    plt.title('Daily Ozone Concentrations in Missouri (Sept-Oct 2001)', fontsize=16)\n",
    "    plt.xlabel('Date', fontsize=13)\n",
    "    plt.ylabel('DS_O3_pred (ppb)', fontsize=13)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Tidy layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96504d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8.2.4 Animated Visualization of Ozone in Missouri ---\n",
    "# This creates an animation\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "import matplotlib.animation as animation\n",
    "import pandas as pd\n",
    "\n",
    "# Filter data for Missouri for the specific date range\n",
    "date_range = pd.date_range(start='2001-09-01', end='2001-10-17')\n",
    "mo_o3_date_filtered = merged_gdf[\n",
    "    (merged_gdf['statefips'] == '29') &\n",
    "    (merged_gdf['date'].isin(date_range))\n",
    "]\n",
    "\n",
    "# Convert to GeoDataFrame for plotting\n",
    "gdf_mo_o3_dates = gpd.GeoDataFrame(\n",
    "    mo_o3_date_filtered,\n",
    "    geometry=gpd.points_from_xy(mo_o3_date_filtered.longitude, mo_o3_date_filtered.latitude),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Set up plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "gdf_missouri_counties.plot(ax=ax, facecolor='#eaeaea', edgecolor='black', linewidth=0.4)\n",
    "\n",
    "ax.set_xlim([-95.6, -89.2])\n",
    "ax.set_ylim([36.4, 40.4])\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel(\"Longitude\", fontsize=13)\n",
    "ax.set_ylabel(\"Latitude\", fontsize=13)\n",
    "ax.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# Initialize scatter with dummy data (must be numeric arrays)\n",
    "scatter = ax.scatter(\n",
    "    [], [], c=[], cmap='OrRd', \n",
    "    vmin=mo_o3_date_filtered['DS_O3_pred'].min(),\n",
    "    vmax=mo_o3_date_filtered['DS_O3_pred'].max(), \n",
    "    edgecolor='black', \n",
    "    linewidth=0.5, \n",
    "    alpha=0.8, \n",
    "    s=[])\n",
    "\n",
    "# colorbar \n",
    "cbar = plt.colorbar(scatter, ax=ax, orientation='horizontal', shrink=0.5, pad=0.05, aspect=30)\n",
    "cbar.set_label(\"DS_O3_pred (ppb)\")\n",
    "\n",
    "title = ax.set_title('', fontsize=16, pad=15)\n",
    "\n",
    "# update function\n",
    "def update(day):\n",
    "    current_date = date_range[day]\n",
    "    daily_data = gdf_mo_o3_dates[gdf_mo_o3_dates['date'] == current_date]\n",
    "\n",
    "    if not daily_data.empty:\n",
    "        xy = np.column_stack([daily_data.geometry.x, daily_data.geometry.y])\n",
    "        sizes = daily_data['DS_O3_pred'] / 5\n",
    "        colors = daily_data['DS_O3_pred']\n",
    "\n",
    "        scatter.set_offsets(xy)\n",
    "        scatter.set_sizes(sizes)\n",
    "        scatter.set_array(colors)\n",
    "    else:\n",
    "        scatter.set_offsets(np.empty((0, 2)))\n",
    "        scatter.set_sizes([])\n",
    "        scatter.set_array([])\n",
    "\n",
    "    title.set_text(f'Ozone Values in Missouri on {current_date.strftime(\"%Y-%m-%d\")}')\n",
    "    return scatter, title\n",
    "\n",
    "# Create and run animation\n",
    "anim = animation.FuncAnimation(\n",
    "    fig, update, frames=len(date_range), interval=800, blit=True, repeat=True\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# For Jupyter notebook, render the animation as HTML\n",
    "from IPython.display import HTML\n",
    "HTML(anim.to_jshtml())\n",
    "\n",
    "# anim.save('missouri_ozone_2001_0910_0917.gif', writer='imagemagick', fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52edc259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 7. Load and Subset Counties Shapefile \n",
    "# Description: Loading county boundary data for potential geospatial analysis.\n",
    "# duplicate cell to correct data \n",
    "# ==============================================================================\n",
    "\n",
    "# Keep only relevant columns (GEOID10, geometry, STATEFP10 renamed to statefips)\n",
    "counties_gdf_subset_2 = counties_gdf[['GEOID10', 'STATEFP10']].rename(columns={'STATEFP10': 'statefips'})\n",
    "\n",
    "# merge ozone data (df_reloaded_01_05) with the shapefile data on GEOID\n",
    "df_reloaded_01_05 = pd.merge(df_reloaded_01_05, counties_gdf_subset_2, left_on='GEOID10', right_on='GEOID10', how='left')\n",
    "\n",
    "# confirm the merge\n",
    "print(df_reloaded_01_05.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 9. Data Capping for 2001-2005 Ozone Data\n",
    "# Description: Applying a cap to the anomalously high DS_O3_pred values identified\n",
    "#              on September 13, 2001, in Missouri, and verifying the effect of this capping\n",
    "# ==============================================================================\n",
    "\n",
    "# Keep only relevant columns (GEOID10, geometry, STATEFP10 renamed to statefips)\n",
    "counties_gdf_subset_2 = counties_gdf[['GEOID10', 'STATEFP10']].rename(columns={'STATEFP10': 'statefips'})\n",
    "\n",
    "# Merge ozone data (df_reloaded_01_05) with the shapefile data on GEOID\n",
    "df_reloaded_01_05 = pd.merge(df_reloaded_01_05, counties_gdf_subset_2, left_on='GEOID10', right_on='GEOID10', how='left')\n",
    "\n",
    "# Display to confirm the merge\n",
    "print(df_reloaded_01_05.head())\n",
    "\n",
    "print(\"\\n--- Applying Data Cap to df_ozone_2001_2005 ---\")\n",
    "# Define the capping conditions\n",
    "# These are specific to the identified anomaly: Missouri (statefips '29'), date 2001-09-13, DS_O3_pred > 150 ppb\n",
    "mask_to_cap = (\n",
    "    (df_reloaded_01_05['statefips'] == '29') &\n",
    "    (df_reloaded_01_05['date'] == pd.Timestamp('2001-09-13')) &\n",
    "    (df_reloaded_01_05['DS_O3_pred'] > 150)\n",
    ")\n",
    "\n",
    "num_values_to_cap = mask_to_cap.sum()\n",
    "print(f\"Number of DS_O3_pred values to be capped at 150 ppb: {num_values_to_cap}\")\n",
    "\n",
    "if num_values_to_cap > 0:\n",
    "    df_reloaded_01_05.loc[mask_to_cap, 'DS_O3_pred'] = 150\n",
    "    print(\"Capping applied.\")\n",
    "\n",
    "    # Verification of the capping effect\n",
    "    print(\"\\nDescriptive statistics for DS_O3_pred in Missouri on 2001-09-13 after capping:\")\n",
    "    verification_df = df_reloaded_01_05[\n",
    "        (df_reloaded_01_05['statefips'] == '29') &\n",
    "        (df_reloaded_01_05['date'] == pd.Timestamp('2001-09-13'))\n",
    "    ]\n",
    "    print(verification_df['DS_O3_pred'].describe())\n",
    "    print(f\"Max value after capping for this subset: {verification_df['DS_O3_pred'].max()}\")\n",
    "else:\n",
    "    print(\"No values met the capping criteria (or data already capped).\")\n",
    "\n",
    "df_reloaded_01_05.drop(['statefips'], axis=1, inplace=True)\n",
    "\n",
    "print(\"\\n----------df_reloaded_01_05-----------------\")\n",
    "print(tabulate(df_reloaded_01_05.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "print(\"Combining the three ozone dataframes\")\n",
    "df_ozone_full = pd.concat(\n",
    "    [df_reloaded_01_05, df_reloaded_06_10, df_reloaded_11_14],\n",
    "    ignore_index=True\n",
    ")\n",
    "print(f\"Shape of combined df_ozone_full: {df_ozone_full.shape}\")\n",
    "print(df_ozone_full.head())\n",
    "df_ozone_full.to_feather(\"/mnt/c/Users/WSTATION/Desktop/GEOAI/project/homework/homework/data/clean_data/ozone_2001_2014.feather\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 10. Data Aggragation for Ozone Data\n",
    "# Description: Aggragating cencus tract ozone to county level \n",
    "#             \n",
    "# ==============================================================================\n",
    "# uncomment the line below if you did not run the cell above and want to load the existing data set \n",
    "# df_ozone_full= pd.read_feather(\"/mnt/c/Users/WSTATION/Desktop/GEOAI/project/homework/homework/data/clean_data/ozone_2001_2014.feather\")\n",
    "\n",
    "print(\"\\n---------- Column Data Types (df_ozone_full) ----------\")\n",
    "print(df_ozone_full.dtypes)\n",
    "\n",
    "# print preview tables\n",
    "print(\"\\n----------df_ozone_full-----------------\")\n",
    "print(tabulate(df_ozone_full.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "# Inspecting DS_O3_pred column in df_ozone_df_ozone_full\n",
    "print(\"--- Inspecting DS_O3_pred column in df_ozone_full ---\")\n",
    "print(f\"Missing values in DS_O3_pred: {df_ozone_full['DS_O3_pred'].isnull().sum()}\")\n",
    "print(\"Descriptive statistics for DS_O3_pred (overall):\")\n",
    "print(df_ozone_full['DS_O3_pred'].describe(percentiles=[.01, .05, .25, .5, .75, .95, .99]))\n",
    "\n",
    "\n",
    "# Verify unique GEOID10 counts per year for df_ozone_full\n",
    "print(\"--- Unique GEOID10 counts per year (df_ozone_full) ---\")\n",
    "print(df_ozone_full.groupby('year')['GEOID10'].nunique())\n",
    "\n",
    "\n",
    "print(\"\\n--- Aggregating Combined Ozone Data to Annual County Level ---\")\n",
    "\n",
    "# make sure 'year' and 'GEOID10' are suitable for grouping\n",
    "if not (pd.api.types.is_numeric_dtype(df_ozone_full['year']) and 'GEOID10' in df_ozone_full.columns):\n",
    "    print(\"ERROR: 'year' is not numeric or 'GEOID10' is missing/incorrect in df_ozone_full.\")\n",
    "    exit()\n",
    "\n",
    "df_ozone_annual_county = df_ozone_full.groupby(\n",
    "    ['year', 'GEOID10']  # Group by year and the 5-digit county FIPS\n",
    ")['DS_O3_pred'].agg(\n",
    "    mean_ozone_pred='mean',\n",
    "    median_ozone_pred='median',\n",
    "    max_ozone_pred='max',\n",
    "    p95_ozone_pred=lambda x: x.quantile(0.95), # 95th percentile\n",
    "    std_ozone_pred='std',\n",
    "    # count_ozone_days=('DS_O3_pred', 'count') # daily tract observations\n",
    ").reset_index()\n",
    "\n",
    "# Rename GEOID10 to 'countyfips' \n",
    "df_ozone_annual_county.rename(columns={'GEOID10': 'countyfips'}, inplace=True)\n",
    "\n",
    "print(\"\\nAggregated Annual County Ozone Data (sample):\")\n",
    "print(tabulate(df_ozone_annual_county.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "print(f\"Shape of aggregated ozone data: {df_ozone_annual_county.shape}\")\n",
    "print(f\"Number of unique county-year observations: {len(df_ozone_annual_county)}\")\n",
    "\n",
    "\n",
    "print(\"\\n---------- Column Data Types (df_ozone_annual_county) ----------\")\n",
    "print(df_ozone_annual_county.dtypes)\n",
    "\n",
    "\n",
    "# print preview tables\n",
    "print(\"\\n----------ddf_ozone_annual_county-----------------\")\n",
    "print(tabulate(df_ozone_annual_county.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "# Inspect DS_O3_pred column in df_ozone_annual_county\n",
    "print(\"--- Inspecting mean_ozone_pred column in df_ozone_annual_county ---\")\n",
    "print(f\"Missing values in mean_ozone_pred: {df_ozone_annual_county['mean_ozone_pred'].isnull().sum()}\")\n",
    "print(\"Descriptive statistics for Dmean_ozone_pred (overall):\")\n",
    "print(df_ozone_annual_county['mean_ozone_pred'].describe(percentiles=[.01, .05, .25, .5, .75, .95, .99]))\n",
    "\n",
    "\n",
    "# Veriy unique GEOID10 counts per year for df_ozone_annual_county\n",
    "print(\"--- Unique countyfips counts per year (df_ozone_full) ---\")\n",
    "print(df_ozone_annual_county.groupby('year')['countyfips'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 10.1 Final Processing and export\n",
    "# Description: fix any errors with countyfips and export\n",
    "#             \n",
    "# ==============================================================================\n",
    "\n",
    "# Filter countyfips with exactly 4 characters\n",
    "# countyfips_4_chars = df_ozone_annual_county[df_ozone_annual_county['countyfips'].str.len() == 4]['countyfips'].unique()\n",
    "\n",
    "# Inspect sorted list for easier review\n",
    "# countyfips_4_chars_sorted = sorted(countyfips_4_chars)\n",
    "\n",
    "# df_ozone_annual_county.to_feather(\"/mnt/c/Users/WSTATION/Desktop/GEOAI/project/homework/homework/data/clean_data/ozone_counties_2001_2014.feather\")\n",
    "\n",
    "# uncomment and run the code below is their are fips with 4 chars \n",
    "# Ensure countyfips is a zero-padded 5-digit string\n",
    "# df_ozone_annual_county['countyfips'] = df_ozone_annual_county['countyfips'].astype(str).str.zfill(5)\n",
    "\n",
    "# # Immediately verify the fix\n",
    "# print(df_ozone_annual_county.head())\n",
    "# print(df_ozone_annual_county['countyfips'].str.len().unique())\n",
    "\n",
    "# df_ozone_annual_county.to_feather(\"/mnt/c/Users/WSTATION/Desktop/GEOAI/project/homework/homework/data/clean_data/ozone_counties_2001_2014.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b19399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 11. CVD Data Processing\n",
    "# Description: Process the CVD data \n",
    "#             \n",
    "# ==============================================================================\n",
    "cvd= pd.read_feather(\"/mnt/c/Users/WSTATION/Desktop/GEOAI_ML/data/cvd.feather\")\n",
    "\n",
    "print(\"\\n---------- Column Data Types (cvd) ----------\")\n",
    "print(cvd.dtypes)\n",
    "\n",
    "# print preview tables\n",
    "print(\"\\n----------cvd-----------------\")\n",
    "print(tabulate(cvd.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "# Verify unique GEOID10 counts per year for df_ozone_annual_county\n",
    "print(\"--- Unique countyfips counts per year (cvd) ---\")\n",
    "print(cvd.groupby('Year')['LocationID'].nunique())\n",
    "\n",
    "print(\"\\nCVD Data Info:\")\n",
    "cvd.info(verbose=True, show_counts=True)\n",
    "print(\"\\nCVD Data Types:\")\n",
    "print(cvd.dtypes)\n",
    "print(\"\\nMissing values in CVD Data:\")\n",
    "print(cvd.isnull().sum()) # check Data_Value\n",
    "\n",
    "\n",
    "# Stratification 1\n",
    "print(f\"\\n-------------Stratification1-------------\") \n",
    "print(f\"\\nUnique GeographicLevels: {cvd['GeographicLevel'].unique()}\") # Expect 'County'\n",
    "print(f\"\\nUnique DataSources: {cvd['DataSource'].unique()}\") # Expect 'NVSS'\n",
    "print(f\"\\nUnique Classes: {cvd['Class'].unique()}\") # Filter for 'Cardiovascular Diseases'\n",
    "print(f\"\\nUnique Topics (sample): {cvd['Topic'].unique()[:20]}\") # To see heart disease types\n",
    "print(f\"\\nUnique Data_Value_Units: {cvd['Data_Value_Unit'].unique()}\")\n",
    "print(f\"\\nUnique Data_Value_Types: {cvd['Data_Value_Type'].unique()}\") # CRITICAL\n",
    "print(f\"\\nUnique StratificationCategory1: {cvd['StratificationCategory1'].unique()}\")\n",
    "print(f\"\\nUnique Stratification1 (sample): {cvd['Stratification1'].unique()[:10]}\")\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Stratification 2 \n",
    "print(f\"\\n-------------Stratification2-------------\") \n",
    "print(f\"\\nUnique GeographicLevels: {cvd['GeographicLevel'].unique()}\") # Expect 'County'\n",
    "print(f\"\\nUnique DataSources: {cvd['DataSource'].unique()}\") # Expect 'NVSS'\n",
    "print(f\"\\nUnique Classes: {cvd['Class'].unique()}\") # Filter for 'Cardiovascular Diseases'\n",
    "print(f\"\\nUnique Topics (sample): {cvd['Topic'].unique()[:20]}\") # To see heart disease types\n",
    "print(f\"\\nUnique Data_Value_Units: {cvd['Data_Value_Unit'].unique()}\")\n",
    "print(f\"\\nUnique Data_Value_Types: {cvd['Data_Value_Type'].unique()}\") # CRITICAL\n",
    "print(f\"\\nUnique StratificationCategory2: {cvd['StratificationCategory2'].unique()}\")\n",
    "print(f\"\\nUnique Stratification2 (sample): {cvd['Stratification2'].unique()[:10]}\")\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Stratification 3\n",
    "print(f\"\\n-------------Stratification3-------------\") \n",
    "print(f\"\\nUnique GeographicLevels: {cvd['GeographicLevel'].unique()}\") # Expect 'County'\n",
    "print(f\"\\nUnique DataSources: {cvd['DataSource'].unique()}\") # Expect 'NVSS'\n",
    "print(f\"\\nUnique Classes: {cvd['Class'].unique()}\") # Filter for 'Cardiovascular Diseases'\n",
    "print(f\"\\nUnique Topics (sample): {cvd['Topic'].unique()[:20]}\") # To see heart disease types\n",
    "print(f\"\\nUnique Data_Value_Units: {cvd['Data_Value_Unit'].unique()}\")\n",
    "print(f\"\\nUnique Data_Value_Types: {cvd['Data_Value_Type'].unique()}\") # CRITICAL\n",
    "print(f\"\\nUnique StratificationCategory3: {cvd['StratificationCategory3'].unique()}\")\n",
    "print(f\"\\nUnique Stratification3 (sample): {cvd['Stratification3'].unique()[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169594b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 11. CVD Data Anomaly Sorting\n",
    "# Description: Sorting out the anomolies in the CVD data\n",
    "#             \n",
    "# ==============================================================================\n",
    "\n",
    "# Count occurrences of \"2010 - 2019\" and \"1999 - 2010\"\n",
    "year_counts = cvd['Year'].value_counts()\n",
    "\n",
    "anomaly_counts = year_counts[['2010 - 2019', '1999 - 2010']]\n",
    "print(anomaly_counts)\n",
    "\n",
    "# Subset with anomalies\n",
    "cvd_year_anomalies = cvd[cvd['Year'].isin(['2010 - 2019', '1999 - 2010'])]\n",
    "\n",
    "# Quick preview\n",
    "print(\"Preview of anomalous year data:\")\n",
    "print(cvd_year_anomalies.head())\n",
    "\n",
    "# Investigate unique values to understand what these records contain\n",
    "print(\"Unique values in 'Data_Value_Type' for anomalies:\")\n",
    "print(cvd_year_anomalies['Data_Value_Type'].unique())\n",
    "\n",
    "print(\"Unique 'Topic' values for anomalies:\")\n",
    "print(cvd_year_anomalies['Topic'].unique())\n",
    "\n",
    "# Check if specific Stratifications or Data_Value_Types are exclusive to these anomalies\n",
    "print(\"Counts per StratificationCategory1 for anomalies:\")\n",
    "print(cvd_year_anomalies['Stratification1'].value_counts())\n",
    "\n",
    "# Filte cvd data to exclude anomalous year ranges\n",
    "cvd_no_ranges = cvd[~cvd['Year'].isin(['2010 - 2019', '1999 - 2010'])].copy()\n",
    "\n",
    "# Verify exclusion worked properly\n",
    "print(\"Unique years after exclusion:\")\n",
    "print(cvd_no_ranges['Year'].unique())\n",
    "\n",
    "print(\"Count of remaining records:\", len(cvd_no_ranges))\n",
    "\n",
    "\n",
    "print(\"\\nUnique Data_Value_Type after exclusion:\")\n",
    "print(cvd_no_ranges['Data_Value_Type'].unique())\n",
    "\n",
    "\n",
    "print(\"\\nUnique Topics after exclusion:\")\n",
    "print(cvd_no_ranges['Topic'].unique())\n",
    "\n",
    "print(\"\\nUnique Stratifications after exclusion:\")\n",
    "print(cvd_no_ranges[['StratificationCategory1', 'StratificationCategory2', 'StratificationCategory3']].drop_duplicates())\n",
    "\n",
    "# Verify unique ountyfips counts per year for df_ozone_annual_county\n",
    "print(\"--- Unique countyfips counts per year (cvd) ---\")\n",
    "print(cvd_no_ranges.groupby('Year')['LocationID'].nunique())\n",
    "\n",
    "cvd_no_ranges['countyfips'] = cvd_no_ranges['LocationID'].astype(str).str.zfill(5)\n",
    "cvd_no_ranges['year'] = cvd_no_ranges['Year'].astype(int)\n",
    "\n",
    "# Counties in Ozone Data\n",
    "ozone_counties = set(df_ozone_annual_county['countyfips'].unique())\n",
    "\n",
    "# Counties in CVD Data\n",
    "cvd_counties = set(cvd_no_ranges['countyfips'].astype(str).str.zfill(5).unique())\n",
    "\n",
    "# Counties in CVD but not in Ozone\n",
    "extra_cvd_counties = cvd_counties - ozone_counties\n",
    "\n",
    "print(\"Counties present in CVD data but not in ozone data:\")\n",
    "print(sorted(extra_cvd_counties))\n",
    "print(f\"Number of extra counties: {len(extra_cvd_counties)}\")\n",
    "\n",
    "\n",
    "print(\"\\n---------- Column Data Types (df_ozone_annual_county) ----------\")\n",
    "print(df_ozone_annual_county.dtypes)\n",
    "\n",
    "\n",
    "# Now print preview tables\n",
    "print(\"\\n----------ddf_ozone_annual_county-----------------\")\n",
    "print(tabulate(df_ozone_annual_county.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "# Inspecting DS_O3_pred column in df_ozone_annual_county\n",
    "print(\"--- Inspecting mean_ozone_pred column in df_ozone_annual_county ---\")\n",
    "print(f\"Missing values in mean_ozone_pred: {df_ozone_annual_county['mean_ozone_pred'].isnull().sum()}\")\n",
    "print(\"Descriptive statistics for Dmean_ozone_pred (overall):\")\n",
    "print(df_ozone_annual_county['mean_ozone_pred'].describe(percentiles=[.01, .05, .25, .5, .75, .95, .99]))\n",
    "\n",
    "\n",
    "# Verify unique GEOID10 counts per year for df_ozone_annual_county\n",
    "print(\"--- Unique countyfips counts per year (df_ozone_full) ---\")\n",
    "print(df_ozone_annual_county.groupby('year')['countyfips'].nunique())\n",
    "\n",
    "\n",
    "print(\"\\nCVD Data Info:\")\n",
    "cvd_no_ranges.info(verbose=True, show_counts=True)\n",
    "print(\"\\nCVD Data Types:\")\n",
    "print(cvd_no_ranges.dtypes)\n",
    "print(\"\\nMissing values in CVD Data:\")\n",
    "print(cvd_no_ranges.isnull().sum()) # check Data_Value\n",
    "\n",
    "\n",
    "# Stratification 1\n",
    "print(f\"\\n-------------Stratification1-------------\") \n",
    "print(f\"\\nUnique GeographicLevels: {cvd_no_ranges['GeographicLevel'].unique()}\") # Expect 'County'\n",
    "print(f\"\\nUnique DataSources: {cvd_no_ranges['DataSource'].unique()}\") # Expect 'NVSS'\n",
    "print(f\"\\nUnique Classes: {cvd_no_ranges['Class'].unique()}\") # Filter for 'Cardiovascular Diseases'\n",
    "print(f\"\\nUnique Topics (sample): {cvd_no_ranges['Topic'].unique()[:20]}\") # To see heart disease types\n",
    "print(f\"\\nUnique Data_Value_Units: {cvd_no_ranges['Data_Value_Unit'].unique()}\")\n",
    "print(f\"\\nUnique Data_Value_Types: {cvd_no_ranges['Data_Value_Type'].unique()}\") # CRITICAL\n",
    "print(f\"\\nUnique StratificationCategory1: {cvd_no_ranges['StratificationCategory1'].unique()}\")\n",
    "print(f\"\\nUnique Stratification1 (sample): {cvd_no_ranges['Stratification1'].unique()[:10]}\")\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Stratification 2 \n",
    "print(f\"\\n-------------Stratification2-------------\") \n",
    "print(f\"\\nUnique GeographicLevels: {cvd_no_ranges['GeographicLevel'].unique()}\") # Expect 'County'\n",
    "print(f\"\\nUnique DataSources: {cvd_no_ranges['DataSource'].unique()}\") # Expect 'NVSS'\n",
    "print(f\"\\nUnique Classes: {cvd_no_ranges['Class'].unique()}\") # Filter for 'Cardiovascular Diseases'\n",
    "print(f\"\\nUnique Topics (sample): {cvd_no_ranges['Topic'].unique()[:20]}\") # To see heart disease types\n",
    "print(f\"\\nUnique Data_Value_Units: {cvd_no_ranges['Data_Value_Unit'].unique()}\")\n",
    "print(f\"\\nUnique Data_Value_Types: {cvd_no_ranges['Data_Value_Type'].unique()}\") # CRITICAL\n",
    "print(f\"\\nUnique StratificationCategory2: {cvd_no_ranges['StratificationCategory2'].unique()}\")\n",
    "print(f\"\\nUnique Stratification2 (sample): {cvd_no_ranges['Stratification2'].unique()[:10]}\")\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Stratification 3\n",
    "print(f\"\\n-------------Stratification3-------------\") \n",
    "print(f\"\\nUnique GeographicLevels: {cvd_no_ranges['GeographicLevel'].unique()}\") # Expect 'County'\n",
    "print(f\"\\nUnique DataSources: {cvd_no_ranges['DataSource'].unique()}\") # Expect 'NVSS'\n",
    "print(f\"\\nUnique Classes: {cvd_no_ranges['Class'].unique()}\") # Filter for 'Cardiovascular Diseases'\n",
    "print(f\"\\nUnique Topics (sample): {cvd_no_ranges['Topic'].unique()[:20]}\") # To see heart disease types\n",
    "print(f\"\\nUnique Data_Value_Units: {cvd_no_ranges['Data_Value_Unit'].unique()}\")\n",
    "print(f\"\\nUnique Data_Value_Types: {cvd_no_ranges['Data_Value_Type'].unique()}\") # CRITICAL\n",
    "print(f\"\\nUnique StratificationCategory3: {cvd_no_ranges['StratificationCategory3'].unique()}\")\n",
    "print(f\"\\nUnique Stratification3 (sample): {cvd_no_ranges['Stratification3'].unique()[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ea9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ## 11. Merge and Export\n",
    "# Description: Merge the Ozone and CVD data and export as feather\n",
    "#             \n",
    "# ==============================================================================\n",
    "# cvd_no_ranges.drop(columns=['Year', 'LocationID'], inplace=True)\n",
    "# cvd_no_ranges['year'] = cvd_no_ranges['year'].astype('int32')\n",
    "print(cvd_no_ranges.dtypes[['countyfips', 'year']])\n",
    "print(cvd_no_ranges.head())\n",
    "\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    df_ozone_annual_county,\n",
    "    cvd_no_ranges,\n",
    "    on=['countyfips', 'year'],\n",
    "    how='inner'  \n",
    ")\n",
    "\n",
    "print(\"Merged Data Shape:\", merged_df.shape)\n",
    "print(merged_df.head())\n",
    "\n",
    "# Verify unique GEOID10 counts per year for merged_df\n",
    "print(\"--- Unique countyfips counts per year (merged_df) ---\")\n",
    "print(merged_df.groupby('year')['countyfips'].nunique())\n",
    "\n",
    "\n",
    "print(\"\\nmerged_df Data Info:\")\n",
    "merged_df.info(verbose=True, show_counts=True)\n",
    "print(\"\\nmerged_df Data Types:\")\n",
    "print(merged_df.dtypes)\n",
    "print(\"\\nMissing values in merged_df:\")\n",
    "print(merged_df.isnull().sum()) # check Data_Value\n",
    "\n",
    "merged_df_clean = merged_df.dropna(subset=['Data_Value']).copy()\n",
    "\n",
    "\n",
    "print(\"Remaining rows after filtering:\", merged_df_clean.shape[0])\n",
    "print(merged_df_clean.isnull().sum())\n",
    "\n",
    "merged_df_clean = merged_df_clean.drop(columns=['Data_Value_Footnote_Symbol', 'Data_Value_Footnote'])\n",
    "print(merged_df_clean.info())\n",
    "\n",
    "print(\"\\n---------- Column Data Types (merged_df_clean) ----------\")\n",
    "print(merged_df_clean.dtypes)\n",
    "\n",
    "merged_df_clean.to_feather(\"/mnt/c/Users/WSTATION/Desktop/GEOAI/project/homework/homework/data/clean_data/ozone_cvd_master.feather\")\n",
    "\n",
    "# Now print preview tables\n",
    "print(\"\\n----------dmerged_df_clean-----------------\")\n",
    "print(tabulate(merged_df_clean.head(), headers='keys', tablefmt='pretty', showindex=False))\n",
    "\n",
    "# Inspecting DS_O3_pred column in merged_df_clean\n",
    "print(\"--- Inspecting mean_ozone_pred column in merged_df_clean ---\")\n",
    "print(f\"Missing values in mean_ozone_pred: {merged_df_clean['mean_ozone_pred'].isnull().sum()}\")\n",
    "print(\"Descriptive statistics for Dmean_ozone_pred (overall):\")\n",
    "print(merged_df_clean['mean_ozone_pred'].describe(percentiles=[.01, .05, .25, .5, .75, .95, .99]))\n",
    "\n",
    "\n",
    "# Verify unique countyfips counts per year for merged_df_clean\n",
    "print(\"--- Unique countyfips counts per year (df_ozone_full) ---\")\n",
    "print(merged_df_clean.groupby('year')['countyfips'].nunique())\n",
    "\n",
    "\n",
    "print(\"\\nCVD Data Info:\")\n",
    "merged_df_clean.info(verbose=True, show_counts=True)\n",
    "print(\"\\nCVD Data Types:\")\n",
    "print(merged_df_clean.dtypes)\n",
    "print(\"\\nMissing values in CVD Data:\")\n",
    "print(merged_df_clean.isnull().sum()) # check Data_Value\n",
    "\n",
    "\n",
    "# Stratification 1\n",
    "print(f\"\\n-------------Stratification1-------------\") \n",
    "print(f\"\\nUnique GeographicLevels: {merged_df_clean['GeographicLevel'].unique()}\") # Expect 'County'\n",
    "print(f\"\\nUnique DataSources: {merged_df_clean['DataSource'].unique()}\") # Expect 'NVSS'\n",
    "print(f\"\\nUnique Classes: {merged_df_clean['Class'].unique()}\") # Filter for 'Cardiovascular Diseases'\n",
    "print(f\"\\nUnique Topics (sample): {merged_df_clean['Topic'].unique()[:20]}\") # To see heart disease types\n",
    "print(f\"\\nUnique Data_Value_Units: {merged_df_clean['Data_Value_Unit'].unique()}\")\n",
    "print(f\"\\nUnique Data_Value_Types: {merged_df_clean['Data_Value_Type'].unique()}\") # CRITICAL\n",
    "print(f\"\\nUnique StratificationCategory1: {merged_df_clean['StratificationCategory1'].unique()}\")\n",
    "print(f\"\\nUnique Stratification1 (sample): {merged_df_clean['Stratification1'].unique()[:10]}\")\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Stratification 2 \n",
    "print(f\"\\n-------------Stratification2-------------\") \n",
    "print(f\"\\nUnique GeographicLevels: {merged_df_clean['GeographicLevel'].unique()}\") # Expect 'County'\n",
    "print(f\"\\nUnique DataSources: {merged_df_clean['DataSource'].unique()}\") # Expect 'NVSS'\n",
    "print(f\"\\nUnique Classes: {merged_df_clean['Class'].unique()}\") # Filter for 'Cardiovascular Diseases'\n",
    "print(f\"\\nUnique Topics (sample): {merged_df_clean['Topic'].unique()[:20]}\") # To see heart disease types\n",
    "print(f\"\\nUnique Data_Value_Units: {merged_df_clean['Data_Value_Unit'].unique()}\")\n",
    "print(f\"\\nUnique Data_Value_Types: {merged_df_clean['Data_Value_Type'].unique()}\") # CRITICAL\n",
    "print(f\"\\nUnique StratificationCategory2: {merged_df_clean['StratificationCategory2'].unique()}\")\n",
    "print(f\"\\nUnique Stratification2 (sample): {merged_df_clean['Stratification2'].unique()[:10]}\")\n",
    "\n",
    "print(\"---------------------------\")\n",
    "\n",
    "# Stratification 3\n",
    "print(f\"\\n-------------Stratification3-------------\") \n",
    "print(f\"\\nUnique GeographicLevels: {merged_df_clean['GeographicLevel'].unique()}\") # Expect 'County'\n",
    "print(f\"\\nUnique DataSources: {merged_df_clean['DataSource'].unique()}\") # Expect 'NVSS'\n",
    "print(f\"\\nUnique Classes: {merged_df_clean['Class'].unique()}\") # Filter for 'Cardiovascular Diseases'\n",
    "print(f\"\\nUnique Topics (sample): {merged_df_clean['Topic'].unique()[:20]}\") # To see heart disease types\n",
    "print(f\"\\nUnique Data_Value_Units: {merged_df_clean['Data_Value_Unit'].unique()}\")\n",
    "print(f\"\\nUnique Data_Value_Types: {merged_df_clean['Data_Value_Type'].unique()}\") # CRITICAL\n",
    "print(f\"\\nUnique StratificationCategory3: {merged_df_clean['StratificationCategory3'].unique()}\")\n",
    "print(f\"\\nUnique Stratification3 (sample): {merged_df_clean['Stratification3'].unique()[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
